---
title: "Scraping the Macedonian Constitutional Court for Insight into its Work"
author: Novica Nakov
output: html_notebook
---

```{r, echo=FALSE}
library(rvest) # CRAN v0.3.5
library(polite) # CRAN v0.1.1
library(tidyverse) # CRAN v1.3.0
library(xml2) # CRAN v1.3.2
library(lubridate) # CRAN v1.7.8 
source("R/functions.R")
```

Before we begin with the code I thought it would be good to explain my motivation for doing this analysis. Recently I won a [case](http://ustavensud.mk/?p=19176) at the Macedonian Constitutional Court. I won't go into the legal details here, but from the outset the challenged provision in the Law on execution (if this is an appropriate translation) seemed wrong i.e. unconstitutional.  When finally the Court agreed it was wrong, I asked my self, why no one else thought of this before, or even better why did the court wait for someone to appeal before it acts. After all the court has mandate to act on it own. and this was not a new law. So with this questions in my head I decided to investigate which court decisions are made after appeals from the public, and which are the courts own doing. Enter `R`.

I've never done web scraping before but in general I am familiar with `[rvest](http://rvest.tidyverse.org/)` and I remembered reading about `[polite](https://github.com/dmi3kno/polite)` some time ago, so I thought these should be the tools to get the judgments in some kind of dataframe for further analysis.


# Part 1: Scraping the Court

## Baby steps into scraping 
Beginning with `polite` I set up a `bow` as explained by [Matt Dray](https://www.rostrum.blog/2019/03/04/polite-webscrape/).

```{r, eval = FALSE}
ustaven_bow <- bow(
  url = "http://ustavensud.mk/",  # base URL
  user_agent = "discindo <https://discindo.org>",  # identify ourselves
  force = TRUE
)
```


The court issues two types of documents, decisions and judgements (and maybe this is not the best legal translation.) These are separate categories on a WordPress site: `?cat=82` and `?cat=83`. So first I count the number of pages in each category. Each page has 10 documents linked from it. This gives us an idea of the scale of the scraping operation and enables the future steps.


```{r, eval=FALSE}
page_count <- function(first_page, bow) {
  
  session <- nod(
    bow = ustaven_bow,
    path = first_page
  )
  
  html_page <- polite::scrape(session)
  
  page_numbers <- html_page %>% 
    rvest::html_nodes(css = "a.page-numbers") %>% 
    rvest::html_attr("href") %>% 
    stringr::str_extract(., pattern = "(\\d)+$") %>% 
    as.numeric()
  
  return(max(page_numbers))
}
```


The function above is called with `sapply` to get a named list for the two categories whose first page URLs are in the list `firts_page`.

```{r, eval = FALSE}

first_page <- list(odluki = "?cat=82", resenija = "?cat=83")

category_page_count <- sapply(first_page, page_count, USE.NAMES = TRUE)
```

Of course, it is possible to see the page count on the website itself. But why spend time clicking. :) 

Then, I create two new lists with all the URLs for the category pages.

```{r, eval = FALSE}
construct_category_urls <- function(page_number, list_url) {
  
  url <- paste0(list_url, "&paged=", page_number)
  
  return(url)
}
```

```{r,eval = FALSE}
cat_odluki_url <- lapply(1:category_page_count[[1]], 
                  construct_category_urls, first_page[[1]])

cat_resenija_url <- lapply(1:category_page_count[[2]], 
                    construct_category_urls, first_page[[2]])
```

This creates lists that look like this:

```
[[1]]
[1] "?cat=82&paged=1"

[[2]]
[1] "?cat=82&paged=2"

[[3]]
[1] "?cat=82&paged=3"
```

where each URL leads to a page containg link to the judgements and decisions.

## Getting the URLs for the individual documents

Next we have to scrape each of the above URLs to get another set of lists. For this we use another function.

```{r, eval = FALSE}
get_judgment_urls <- function(url, bow) {
  
  session <- nod(
    bow = bow,
    path = url
  )
  
  html_page <- scrape(session)

  page_urls <- html_page %>% 
    html_nodes(css = "div:nth-child(1) > header:nth-child(1) > h3:nth-child(1) > a:nth-child(1)") %>% 
    html_attr("href")  %>% 
    stringr::str_replace(., pattern = "http://ustavensud.mk/", replacement = "")
    # the str_replace is here to remove the base url alreay in bow()
  
  return(page_urls)
}
```

```{r, eval = FASLE}
urls_odluki <- lapply(cat_odluki_url, get_judgement_urls, ustaven_bow) %>% 
  unlist()

urls_resenija <- lapply(cat_resenija_url, get_judgement_urls, ustaven_bow) %>% 
  unlist()
```

This returns another set of lists that look like this:
```
[1] "?p=19246" "?p=19255" "?p=19241" "?p=19180"
[5] "?p=19176" "?p=18912" "?p=19112" "?p=18758"
[9] "?p=18756" "?p=18412" "?p=19246" "?p=19255"
```
The reason why we truncate the URL like this all the time is that the base URL is already stored in the `bow()`.


## Getting the text of the individual documents

Finaly, we use another function to get the text of the judgements and decisions.

```{r, eval = FALSE}
get_judgement_text <- function(url, bow){
  
  session <- nod(
    bow = bow,
    path = url
  )

  html_page <- scrape(session)

  text <- html_page %>% 
    html_nodes(css = "p") %>% 
    html_text(trim = TRUE) 
  
  return(text) #[3:4] not sure abut subsetting here. maybe keep the whole text.
}
```

```{r, eval = FALSE}
text_odluki <- lapply(urls_odluki, get_judgement_text, bow_ustaven) 

text_resenija <- lapply(urls_resenija, get_judgement_text, bow_ustaven) 
```


Now it is a good time to save the data. I don't want to rerun the scraping every time I need to work on the text -- or better said, court decisions come at a rate of few in a month, and scraping daily as we are digging through the text makes little sense.

I decided to use the RDS format to store the data. I think I read somewhere that RDS is useful because of compression. And since I am pushing the code to Github I wanted to be on the save side. If someone wants a csv I can provide that.

```{r, eval = FALSE}
saveRDS(urls_odluki, "data/urls_odluki.rds")
saveRDS(text_odluki, "data/text_odluki.rds")
saveRDS(urls_resenija, "data/urls_resenija.rds")
saveRDS(text_resenija, "data/text_resenija.rds")
```


# Part II: Datawrangling for the desired format

##Example first

With scraping done I was faced with a problem of how to store the scraped data, i.e, what `class` should the data be. Obviously the first choice is just keep the lists as is. This would be a good approach to access specific paragraphs of the text of the documents knowing that they have somewhat fixed structure. For example paragraph 3 is always about who started (submitted) the appeal (which is what we were interested in from the beginning). 

But, it might be useful to have a data frame that will have the URLs as well. That way we will know which text comes from which URL.

The dataframe however poses as some kind of challenge, because there are several ways in which we can construct it and it will have different structure. Let's try to illustrate this with a simple example.

Given the format of our lists:

```{r}
list_a <- list("url1", "url2", "url3")
list_b <- list(list("a", "aa", "aaa"), list("b", "bb", "bbb", "bbbb"), list("c", "cc"))
```

We can use:

```{r}
tibble(url = list_a, text = list_b)
```
And this returns a tibble with the columns as lists. We can easily `unnest()`, but we need to keep some kind of structure in the second column, because we want to get to that third paragraph quickly. 

This gets us half way there:

```{r}
tibble(url = list_a, text = list_b) %>% 
  unnest(url)
```

But a long format tibble with a column for the paragraph number would be really neat.

```{r}
 tibble(url = list_a, text = list_b) %>% 
  unnest(url) %>% 
  unnest_longer(text) %>% 
  group_by(url) %>% 
  mutate(p_number = row_number()) %>% 
  ungroup()
```

This way we can easily select all third paragraphs using `filter`. But there is one more thing missing in the dataset (well at least for now), and that is the date. 

The date is at the end of each document in a paragraph that looks like this:

```
[1] "У.бр.115/2019\n14 мај 2020 година\nС к о п ј е"
```

So first we need to find this paragraph -- different documents have different lengths so the p_number would be different. And then we need to extract the part containing the date, and convert it to type `date`.

Lest change our test lists so they include this paragraph.

```{r}
list_a <- list("url1", "url2", "url3")
list_b <- list(list("a", "aa", "aaa", "У.бр.115/2019\n14 мај 2020 година\nС к о п ј е"), 
          list("b", "bb", "bbb", "bbbb", "У.бр.115/2020\n15 јуни 2020 година\nС к о п ј е"), 
          list("c", "cc", "У.бр.112/2018\n1 март 2020 година\nС к о п ј е"))
```

And create a long format tibble as above:

```{r}
(df <- tibble(url = list_a, text = list_b) %>% 
  unnest(url) %>% 
  unnest_longer(text) %>% 
  group_by(url) %>% 
  mutate(p_number = row_number()) %>% 
  ungroup())
```

Now we need to find each paragraph containing the date and stack it in a new column called `date`. It's still not a real date, but we are getting there.


```{r}
df %>% 
  group_by(url) %>% 
  mutate(date = text[str_detect(text, "С к о п ј е")]) %>% 
  ungroup()
```

Next we extract the part of the string containing the date. First using `str_split` we split the string into parts ending with newline `\n`, then subset the needed element with [[1]][2].

After that with we clean up the date string with `str_remove`. 

```{r}
(df <- df %>% 
  group_by(url) %>% 
  mutate(date = text[str_detect(text, "С к о п ј е")]) %>% 
  mutate(date = str_split(date, "\n")[[1]][2]) %>% 
  mutate(date = str_remove(date, "година")) %>% 
  ungroup())
```


This leaves us with a date column that makes sense, but is still in the `character` format. So we replace all the Macedonian months with English and `mutate` with `lubridate`. 

```{r}
(df <- df %>% 
   group_by(url) %>% 
      mutate(date = str_replace(date, "март", "March")) %>%
      mutate(date = str_replace(date, "мај", "May")) %>%
      mutate(date = str_replace(date, "јуни", "June")) %>%
      mutate(date = dmy(date)) %>% 
   ungroup()
   )
```

Finally, a dataframe that looks like it hass all the variables we want for this analysis.

## The actual data

Now we need to apply all these transformations to our original data. Here we go. Load what we have saved.

```{r}
all_rds <- list.files(path = "data", pattern = "*.rds", full.names = TRUE)

text_odluki <- readRDS(all_rds[1])
text_resenija <- readRDS(all_rds[2])
urls_odluki <- readRDS(all_rds[3])
urls_resenija <- readRDS(all_rds[4])
```

We verify the pairs are of same lenght.

```{r, eval = FALSE}
all.equal(length(urls_odluki), length(text_odluki))

all.equal(length(urls_resenija), length(text_resenija))
```

And then we create our tibbles.

```{r}
odluki <- tibble(url = urls_odluki, text = text_odluki)
resenija <- tibble(url = urls_resenija, text = text_resenija)
```

Verify we have all observations.

```{r}
all.equal(nrow(odluki), length(urls_odluki))
all.equal(nrow(resenija), length(urls_resenija))
```

And remove the original lists from memory.

```{r}
remove(text_odluki)
remove(text_resenija)
remove(urls_odluki)
remove(urls_resenija)
```


Now we need to apply a function to the tibbles that will do all the transformations. We have all the code above, now we just to wrap it in a function.

```{r, eval = FALSE}

transform_court <- function(tibble) {
  tibble_long <- tibble %>% 
    #unnest(url) %>% 
    unnest_longer(text) %>% 
    group_by(url) %>%
    mutate(p_number = row_number()) %>%
    mutate(date = text[str_detect(text, "С к о п ј е")]) %>%
    mutate(date = str_split(date, "\n")[[1]][2]) %>%
    mutate(date = str_remove(date, "година")) %>%
    mutate(date = str_replace(date, "јануари", "January")) %>%
    mutate(date = str_replace(date, "февруари", "February")) %>%
    mutate(date = str_replace(date, "март", "March")) %>%
    mutate(date = str_replace(date, "април", "April")) %>%
    mutate(date = str_replace(date, "мај", "May")) %>%
    mutate(date = str_replace(date, "јуни", "June")) %>%
    mutate(date = str_replace(date, "јули", "July")) %>%
    mutate(date = str_replace(date, "август", "August")) %>%
    mutate(date = str_replace(date, "септември", "September")) %>%
    mutate(date = str_replace(date, "октомври", "October")) %>%
    mutate(date = str_replace(date, "ноември", "November")) %>%
    mutate(date = str_replace(date, "декември", "December")) %>%
    mutate(date = dmy(date)) %>%
    ungroup()
  
  return(tibble_long)
}
```

And then run.

```{r eval = FALSE}

odluki_long <- transform_court(odluki)

resenija_long <- transform_court(resenija)
```

Let's see what we got.

```{r}
glimpse(odluki_long)
```

```{r}
glimpse(resenija_long)
```

## Part III: Analyzing the Court's documents.

```{r, echo = FALSE}
odluki <- readRDS("data/odluki.rds")
resenija <- readRDS("data/resenija.rds")
```

I loaded the RDS files as `odluki` and `resenija` (I am using Macedonian words, because anyway I am not sure of the Enlgish legal translation).

First I think we should look at the structure of a document. 

```{r}
one <- odluki %>% 
  group_by(url) %>%
  nest(data = c(text))

%>% 
  slice_head(n = 1) %>%
  ungroup() 

one
```



